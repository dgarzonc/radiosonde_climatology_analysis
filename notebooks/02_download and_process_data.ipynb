{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116c686a-5bb2-4fa7-a00f-33067cae7058",
   "metadata": {},
   "source": [
    "<h1> IGRA2 Radiosonde Data Processing </h1>\n",
    "<br>\n",
    "\n",
    "This notebook implements an optimized pipeline for processing radiosonde data from the Integrated Global Radiosonde Archive Version 2 (IGRA2).\n",
    "<br>\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "IGRA2 raw data files present several computational challenges:\n",
    "- Large file sizes\n",
    "- Complex concatenated format with variable-length records\n",
    "- Irregular pressure levels across soundings\n",
    "\n",
    "**Solution Overview**\n",
    "\n",
    "The pipeline addresses these challenges through:\n",
    "- Automated data acquisition: Programmatic download and extraction of station files\n",
    "- Parallel processing: Multi-core disaggregation and formatting\n",
    "- Pressure level standardization: Semi-logarithmic interpolation to regular 5 hPa intervals (1010-100 hPa)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0952f41-0072-4f3e-b92d-59eb9efd5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the parameters in this cell according to your requirements\n",
    "\n",
    "# ID of selected station\n",
    "ID = 'COM00080222'\n",
    "\n",
    "# Period of climatology\n",
    "clim_start_year = 1991\n",
    "clim_end_year = 2020\n",
    "clim_hour = 12  # UTC hour of sonde release\n",
    "\n",
    "# Path of the repository folder\n",
    "path_repo = '/home/david/radiosonde_climatology_analysis/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad896fa5-27bc-455d-8a87-4496174f9373",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a15c20-dc92-4c8b-801a-c92b21f32a4e",
   "metadata": {},
   "source": [
    "# 0. Preamble (functions and libs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18887de9-ec06-4819-a4cc-f3e70a13d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "### folders' paths\n",
    "\n",
    "docs_folder = f'{path_repo}/docs/'\n",
    "station_folder = f'{path_repo}/data/{ID}/'\n",
    "raw_folder = f'{station_folder}/raw/'\n",
    "separated_folder = f'{station_folder}/separated/'\n",
    "interpolated_folder = f'{station_folder}/interpolated/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d93466-1096-4fdb-a820-71002f6b7401",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions and libs\n",
    "\n",
    "############\n",
    "### libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "import os\n",
    "import zipfile\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "from metpy import calc as mpcalc\n",
    "from metpy.units import units\n",
    "\n",
    "\n",
    "############\n",
    "### basic functions to print and habdle txt files\n",
    "\n",
    "def print_(string):\n",
    "    string = f\"{datetime.now().strftime('[%Y-%m-%d %H:%M:%S]')}: {string}\"\n",
    "    print(string)\n",
    "\n",
    "def download_file(url, download_folder):\n",
    "    file_name = url.split(\"/\")[-1]\n",
    "    file_path = os.path.join(download_folder, file_name)\n",
    "    print_(f\"Start to download: {file_name}\")\n",
    "    with requests.get(url, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        with open(file_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    print_(f\"File downloaded: {file_path}\")\n",
    "\n",
    "def unzip_and_remove(zip_file_path, extract_to_folder):\n",
    "    print_(f\"Start to unzip: {zip_file_path}\")\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to_folder)\n",
    "    os.remove(zip_file_path)    \n",
    "    print_(f\"Unzipped and removed: {zip_file_path}\")\n",
    "\n",
    "def get_total_lines(filename):\n",
    "    \"\"\"Efficiently count total lines in file\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        lines = 0\n",
    "        buf_size = 1024 * 1024\n",
    "        read_f = f.raw.read        \n",
    "        buf = read_f(buf_size)\n",
    "        while buf:\n",
    "            lines += buf.count(b'\\n')\n",
    "            buf = read_f(buf_size)        \n",
    "        return lines\n",
    "\n",
    "def str_timediff(time_diff):\n",
    "    total_seconds = time_diff.total_seconds()\n",
    "    # Calculate hours, minutes, and seconds\n",
    "    hours = int(total_seconds // 3600)\n",
    "    minutes = int((total_seconds % 3600) // 60)\n",
    "    seconds = int(total_seconds % 60)\n",
    "    # Print the result\n",
    "    return( f\"Tiempo transcurrido: {hours} horas, {minutes} minutos, {seconds} segundos.\")\n",
    "    \n",
    "def get_lines_starting_with_(file_path, prefix):\n",
    "    line_numbers = []\n",
    "\n",
    "    # Open the file in read mode\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Iterate through each line\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            # Check if the line starts with '#'\n",
    "            if line.startswith(prefix):\n",
    "                line_numbers.append(line_number)\n",
    "    return line_numbers\n",
    "\n",
    "def get_line_from_file(file_path, line_number):\n",
    "    # Open the file in read mode\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Iterate through each line\n",
    "        for current_line_number, line in enumerate(file, start=1):\n",
    "            # Check if the current line number matches the desired line number\n",
    "            if current_line_number == line_number:\n",
    "                return line.rstrip()  # Return the line without trailing whitespaces\n",
    "\n",
    "    # If the desired line number is not found, return an empty string\n",
    "    return ''\n",
    "\n",
    "############\n",
    "### Creating the station's folders\n",
    "os.makedirs(station_folder, exist_ok = True)\n",
    "os.makedirs(separated_folder, exist_ok = True)\n",
    "os.makedirs(interpolated_folder, exist_ok = True)\n",
    "\n",
    "############\n",
    "### Format info of the data in the rawfile, based on IGRA2 documentation\n",
    "df_HeaderRecordFormat = pd.read_csv(f'{docs_folder}/format_header_record.csv')\n",
    "colspecs_header = []\n",
    "header_vars = []\n",
    "for idx in df_HeaderRecordFormat.index:\n",
    "    header_vars += [df_HeaderRecordFormat.loc[idx, 'variable']] \n",
    "    idx_ini_i = df_HeaderRecordFormat.loc[idx, 'start_icol']\n",
    "    idx_end_i = df_HeaderRecordFormat.loc[idx, 'end_icol']\n",
    "    colspecs_header += [(idx_ini_i, idx_end_i)]\n",
    "\n",
    "df_DataRecordFormat = pd.read_csv(f'{docs_folder}/format_data_record.csv')\n",
    "colspecs_data = []\n",
    "for idx in df_DataRecordFormat.index:\n",
    "    idx_ini_i = df_DataRecordFormat.loc[idx, 'start_icol']\n",
    "    idx_end_i = df_DataRecordFormat.loc[idx, 'end_icol']\n",
    "    colspecs_data += [(idx_ini_i, idx_end_i)]\n",
    "\n",
    "############\n",
    "# functions for extract anf format each sonde data\n",
    "def read_file_once_get_line_positions(rawfile):\n",
    "    \"\"\"Read file once to get byte positions of each line start\"\"\"\n",
    "    line_positions = [0]  # First line starts at position 0\n",
    "    \n",
    "    with open(rawfile, 'rb') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            line_positions.append(f.tell())\n",
    "    \n",
    "    return line_positions[:-1]  # Remove the last position (EOF)\n",
    "\n",
    "# to get the metadata of each sonde\n",
    "def extract_headers_optimized(rawfile, lines_start_sondes, colspecs_header, header_vars):\n",
    "    \"\"\"Extract all headers in a single file pass\"\"\"\n",
    "    line_positions = read_file_once_get_line_positions(rawfile)\n",
    "    df_sondes_list = []\n",
    "    \n",
    "    with open(rawfile, 'r') as file:\n",
    "        for i_sounding, start_line in enumerate(lines_start_sondes):\n",
    "            # Seek directly to byte position instead of reading line by line\n",
    "            file.seek(line_positions[start_line - 1])\n",
    "            \n",
    "            # Read just the header line\n",
    "            header_line = file.readline()\n",
    "            \n",
    "            # Parse the header using StringIO for pandas\n",
    "            from io import StringIO\n",
    "            df_headeri = pd.read_fwf(\n",
    "                StringIO(header_line),\n",
    "                colspecs=colspecs_header,\n",
    "                names=header_vars,\n",
    "                nrows=1\n",
    "            )\n",
    "            df_headeri.index = [i_sounding]\n",
    "            df_sondes_list.append(df_headeri)\n",
    "    \n",
    "    return pd.concat(df_sondes_list)\n",
    "\n",
    "### function to extract and format each sondes\n",
    "def process_single_sonde(args):\n",
    "    \"\"\"Process and format a single sonde - designed for parallel execution\"\"\"\n",
    "    (rawfile, i_sounding, start_line, end_line, colspecs_data, \n",
    "     data_vars, separated_folder, ID, sonde_date) = args\n",
    "    \n",
    "    # Read the specific section of the file\n",
    "    with open(rawfile, 'r') as file:\n",
    "        df_sondei = pd.read_fwf(\n",
    "            file,\n",
    "            colspecs=colspecs_data,\n",
    "            names=data_vars,\n",
    "            skiprows=start_line - 1,\n",
    "            nrows=end_line - start_line,\n",
    "            dtype=str\n",
    "        )\n",
    "    \n",
    "    # Convert string columns to numeric where needed\n",
    "    numeric_cols = ['PRESS', 'TEMP', 'DPDP', 'RH', 'WSPD', 'WDIR']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_sondei.columns:\n",
    "            df_sondei[col] = pd.to_numeric(df_sondei[col], errors='coerce')\n",
    "    \n",
    "    # Sonde data formatting based on the IGRA2 documentation\n",
    "    df_sondei.replace(-9999, np.nan, inplace=True)\n",
    "    df_sondei.replace(-8888, np.nan, inplace=True)\n",
    "    \n",
    "    # Apply scaling factors\n",
    "    if 'PRESS' in df_sondei.columns:\n",
    "        df_sondei['PRESS'] /= 100\n",
    "    if 'TEMP' in df_sondei.columns:\n",
    "        df_sondei['TEMP'] /= 10\n",
    "    if 'DPDP' in df_sondei.columns:\n",
    "        df_sondei['DPDP'] /= 10\n",
    "    if 'RH' in df_sondei.columns:\n",
    "        df_sondei['RH'] /= 10\n",
    "    if 'WSPD' in df_sondei.columns:\n",
    "        df_sondei['WSPD'] /= 10\n",
    "    \n",
    "    # Calculate dew point temperature\n",
    "    if 'TEMP' in df_sondei.columns and 'DPDP' in df_sondei.columns:\n",
    "        df_sondei['DEW'] = df_sondei['TEMP'] - df_sondei['DPDP']\n",
    "    \n",
    "    # Calculate missing atmospheric variables using global functions\n",
    "    if 'RH' in df_sondei.columns and 'DEW' in df_sondei.columns and 'TEMP' in df_sondei.columns:\n",
    "        if df_sondei['RH'].count() == 0 and df_sondei['DEW'].count() != 0:\n",
    "            df_sondei['RH'] = calculate_relative_humidity(df_sondei['TEMP'].to_numpy(), df_sondei['DEW'].to_numpy())\n",
    "        if df_sondei['RH'].count() != 0 and df_sondei['DEW'].count() == 0:\n",
    "            df_sondei['DEW'] = calculate_dew_point(df_sondei['TEMP'].to_numpy(), df_sondei['RH'].to_numpy())\n",
    "    \n",
    "    # Calculate wind components\n",
    "    if 'WSPD' in df_sondei.columns and 'WDIR' in df_sondei.columns:\n",
    "        df_sondei['U'] = -df_sondei['WSPD'] * np.sin(df_sondei['WDIR'] * np.pi / 180)\n",
    "        df_sondei['V'] = -df_sondei['WSPD'] * np.cos(df_sondei['WDIR'] * np.pi / 180)\n",
    "    \n",
    "    # Save formatted data to CSV\n",
    "    output_file = f'{separated_folder}/{ID}_{sonde_date.strftime(\"%Y%m%d%H%M\")}.csv'\n",
    "    df_sondei.to_csv(output_file, index=False)\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "### function to disaggregate and format all sondes in a parallel way\n",
    "def process_sondes_optimized(rawfile, lines_start_sondes, colspecs_header, header_vars,\n",
    "                           colspecs_data, df_DataRecordFormat, df_HeaderRecordFormat,\n",
    "                           separated_folder, ID, max_workers=4):\n",
    "    \"\"\"Main optimized processing function\"\"\"\n",
    "    \n",
    "    # Step 1: Extract all headers efficiently\n",
    "    print_(\"Extracting headers...\")\n",
    "    df_sondes = extract_headers_optimized(rawfile, lines_start_sondes, colspecs_header, header_vars)\n",
    "    df_sondes = df_sondes[df_HeaderRecordFormat['variable'].to_list()]\n",
    "    \n",
    "    # Step 2: Prepare arguments for parallel processing\n",
    "    print_(\"Preparing sonde processing tasks...\")\n",
    "    processing_args = []\n",
    "    \n",
    "    for i_sounding in range(len(lines_start_sondes)):\n",
    "        # Extract date info\n",
    "        date_i = datetime(\n",
    "            int(df_sondes.loc[i_sounding, 'YEAR']),\n",
    "            int(df_sondes.loc[i_sounding, 'MONTH']),\n",
    "            int(df_sondes.loc[i_sounding, 'DAY']),\n",
    "            int(df_sondes.loc[i_sounding, 'HOUR'])\n",
    "        )\n",
    "\n",
    "        # Exclude data outside the defined climatology period\n",
    "        if (date_i.year < clim_start_year) or (date_i.year > clim_end_year) or (date_i.hour != clim_hour): continue\n",
    "        \n",
    "        # Calculate line ranges\n",
    "        start_line = lines_start_sondes[i_sounding] + 1\n",
    "        if i_sounding != len(lines_start_sondes) - 1:\n",
    "            end_line = lines_start_sondes[i_sounding + 1]\n",
    "        else:\n",
    "            end_line = get_total_lines(rawfile)\n",
    "        \n",
    "        args = (rawfile, i_sounding, start_line, end_line, colspecs_data,\n",
    "                df_DataRecordFormat['variable'].to_list(), separated_folder, ID, date_i)\n",
    "        processing_args.append(args)\n",
    "    \n",
    "    # Step 3: Process sondes in parallel\n",
    "    print_(f\"Processing {len(processing_args)} sondes with {max_workers} workers...\")\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_sonde = {\n",
    "            executor.submit(process_single_sonde, args): i \n",
    "            for i, args in enumerate(processing_args)\n",
    "        }\n",
    "        \n",
    "        # Process completed tasks\n",
    "        completed = 0\n",
    "        total_sondes = len(processing_args)\n",
    "        last_printed_percentage = 0\n",
    "        \n",
    "        for future in as_completed(future_to_sonde):\n",
    "            try:\n",
    "                output_file = future.result()\n",
    "                completed += 1\n",
    "                \n",
    "                # Calculate percentage and print only at 10% intervals\n",
    "                current_percentage = (completed * 100) // total_sondes\n",
    "                if current_percentage >= last_printed_percentage + 10 and current_percentage <= 100:\n",
    "                    print_(f\"Progress: {current_percentage}% ({completed}/{total_sondes} sondes)\")\n",
    "                    last_printed_percentage = current_percentage\n",
    "                    \n",
    "            except Exception as exc:\n",
    "                sonde_idx = future_to_sonde[future]\n",
    "                print_(f'Sonde {sonde_idx} generated an exception: {exc}')\n",
    "    \n",
    "    print_(\"Processing complete!\")\n",
    "    return df_sondes\n",
    "\n",
    "### function for calculate some meteorological variables\n",
    "def calculate_relative_humidity(temp, dew_point):\n",
    "    temp = temp * units.degC\n",
    "    dew_point = dew_point * units.degC\n",
    "    rh = mpcalc.relative_humidity_from_dewpoint(temp, dew_point)\n",
    "    return rh.magnitude * 100\n",
    "\n",
    "def calculate_dew_point(temp, rh_percent):\n",
    "    temp = temp * units.degC\n",
    "    rh = (rh_percent / 100) * units.dimensionless\n",
    "    dew_point = mpcalc.dewpoint_from_relative_humidity(temp, rh)\n",
    "    return dew_point.magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1744c2e7-61f2-4aec-b7ce-1221f9f39c44",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4d3551-7ba8-4511-bd95-8ce8aea03301",
   "metadata": {},
   "source": [
    "# 1. Read metadata info of selected station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca33da0-685b-4f9f-ae21-7be8f564a236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                COM00080222\n",
       "lat                       4.7\n",
       "lon                    -74.15\n",
       "height                 2547.0\n",
       "name          BOGOTA/ELDORADO\n",
       "start_year               1960\n",
       "end_year                 2025\n",
       "n_sondes                26176\n",
       "Name: 692, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Open metadata file\n",
    "\n",
    "df_DataFormat = pd.read_csv(f'{docs_folder}/format_stations_metadata.csv')\n",
    "colspecs = []\n",
    "selected_variables = df_DataFormat['col_name'].to_list()\n",
    "for i_var in df_DataFormat.index:\n",
    "    var_i = df_DataFormat.loc[i_var, 'col_name']\n",
    "    if not(var_i in selected_variables): continue\n",
    "    idx_ini_i = df_DataFormat.loc[i_var, 'start_icol']\n",
    "    idx_end_i = df_DataFormat.loc[i_var, 'end_icol']\n",
    "    colspecs += [(idx_ini_i, idx_end_i)]\n",
    "\n",
    "metadatafile = f'{docs_folder}/igra2-station-list.txt'\n",
    "with open(metadatafile) as file:\n",
    "    df_stations = pd.read_fwf(metadatafile, colspecs=colspecs, header=None, names=selected_variables)\n",
    "\n",
    "index_station = df_stations.loc[df_stations['id']==ID].index[0]\n",
    "\n",
    "df_stations.loc[index_station]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07ccf2-4eb4-4cda-8f92-af8d3b212a6f",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f9f1f-87a1-4b43-8542-c9d517286fba",
   "metadata": {},
   "source": [
    "# 2. Download Raw Sonde Data from IGRA2 server\n",
    "\n",
    "This section handles the automated retrieval of IGRA2 sonde data files. It consists of two main steps:\n",
    "\n",
    "1. **Download**: Constructs the download URL based on the station ID and retrieves the corresponding `.zip` file containing the raw sonde data.\n",
    "\n",
    "2. **Extraction**: Once downloaded, the `.zip` file is extracted to retrieve the raw text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50dc1b7e-aed5-467d-bb9f-d16103ac598a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-28 11:04:15]: Start to download the data file from IGRA2 server\n",
      "[2025-07-28 11:04:15]: Start to download: COM00080222-data.txt.zip\n",
      "[2025-07-28 11:04:18]: File downloaded: /home/david/radiosonde_climatology_analysis//data/COM00080222//raw/COM00080222-data.txt.zip\n",
      "[2025-07-28 11:04:18]: Download completed successfully\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Download sounding data\n",
    "\n",
    "url_base = 'https://www.ncei.noaa.gov/data/integrated-global-radiosonde-archive/'\n",
    "\n",
    "print_('Start to download the data file from IGRA2 server')\n",
    "url_data = f'{url_base}/access/data-por/{ID}-data.txt.zip'\n",
    "os.makedirs(raw_folder, exist_ok = True)\n",
    "\n",
    "try:\n",
    "    download_file(url_data, raw_folder)\n",
    "    print_('Download completed successfully')\n",
    "    \n",
    "except HTTPError as e:\n",
    "    if e.response.status_code == 503:\n",
    "        print_(\"IGRA2 server is offline\")\n",
    "        print_(f\"ERROR: {str(e)}\")\n",
    "        print_(f\"Server link: {url_base}\")\n",
    "        print_(\"Try again later...\")\n",
    "    else:\n",
    "        raise  # Re-raise other HTTP errors\n",
    "print('\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb60da4-ed3c-45c7-83eb-633bbf376a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-28 11:04:18]: Start to unzip data file\n",
      "[2025-07-28 11:04:18]: Start to unzip: /home/david/radiosonde_climatology_analysis//data/COM00080222//raw//COM00080222-data.txt.zip\n",
      "[2025-07-28 11:04:18]: Unzipped and removed: /home/david/radiosonde_climatology_analysis//data/COM00080222//raw//COM00080222-data.txt.zip\n",
      "[2025-07-28 11:04:18]: Unzip completed successfully\n"
     ]
    }
   ],
   "source": [
    "print_('Start to unzip data file')\n",
    "try:\n",
    "    unzip_and_remove(f'{raw_folder}/{ID}-data.txt.zip', raw_folder)\n",
    "except:\n",
    "    print_(f'Error in file: {raw_folder}/{ID}-data.txt.zip')\n",
    "\n",
    "print_('Unzip completed successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505c5e9-45ea-43d5-b93a-e2327222d32d",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43a2ddd-76fb-4e60-bd8a-d9c45fce7c37",
   "metadata": {},
   "source": [
    "# 3. Disaggregation and Formatting of Sonde Data\n",
    "\n",
    "This section implements an optimized method to split and format individual IGRA2 sonde records from a large raw file. By combining disaggregation and formatting into a single parallel pipeline, the implementation eliminates performance bottlenecks found in traditional sequential workflows.\n",
    "\n",
    "**Key Optimizations:**\n",
    "- Parallel Processing: Uses multiple CPU cores to handle each sonde independently\n",
    "- Single-Pass Logic: Extracts and formats data in a unified step\n",
    "\n",
    "**Process Flow**:\n",
    "1. Header Extraction: Reads all sonde headers in one pass\n",
    "2. Parallel Execution: Assigns each sonde to a worker for processing\n",
    "3. In-Line Formatting: Applies IGRA2 scaling, units, and calculations as it extracts\n",
    "\n",
    "**Functions:**\n",
    "- `extract_headers_optimized()`: Fast header scan using file positioning\n",
    "- `process_single_sonde()`: Handles disaggregation and formatting for one sonde\n",
    "- `process_sondes_optimized()`: Orchestrates full parallel pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6c8182-7b09-4609-9714-36f986fd1b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-28 11:04:18]: Starting to disaggregate the rawfile of BOGOTA/ELDORADO (COM00080222)\n",
      "[2025-07-28 11:04:18]: It could take several minutes...\n",
      "[2025-07-28 11:04:18]: Extracting headers...\n",
      "[2025-07-28 11:04:36]: Preparing sonde processing tasks...\n",
      "[2025-07-28 11:04:36]: Processing 8831 sondes with 11 workers...\n",
      "[2025-07-28 11:06:12]: Progress: 10% (884/8831 sondes)\n",
      "[2025-07-28 11:08:11]: Progress: 20% (1767/8831 sondes)\n",
      "[2025-07-28 11:10:15]: Progress: 30% (2650/8831 sondes)\n",
      "[2025-07-28 11:12:28]: Progress: 40% (3533/8831 sondes)\n",
      "[2025-07-28 11:14:56]: Progress: 50% (4416/8831 sondes)\n",
      "[2025-07-28 11:17:49]: Progress: 60% (5299/8831 sondes)\n",
      "[2025-07-28 11:20:57]: Progress: 70% (6182/8831 sondes)\n",
      "[2025-07-28 11:24:23]: Progress: 80% (7065/8831 sondes)\n",
      "[2025-07-28 11:28:13]: Progress: 90% (7948/8831 sondes)\n",
      "[2025-07-28 11:32:29]: Progress: 100% (8831/8831 sondes)\n",
      "[2025-07-28 11:32:29]: Processing complete!\n",
      "[2025-07-28 11:32:29]: Completed disaggregation of the rawfile of BOGOTA/ELDORADO (COM00080222)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "station = df_stations.loc[index_station, 'name']\n",
    "rawfile = f'{raw_folder}/{ID}-data.txt'\n",
    "\n",
    "print_(f'Starting to disaggregate the rawfile of {station} ({ID})')\n",
    "print_('It could take several minutes...')\n",
    "\n",
    "lines_start_sondes = get_lines_starting_with_(rawfile,'#')\n",
    "\n",
    "df_sondes = process_sondes_optimized(\n",
    "    rawfile = rawfile,\n",
    "    lines_start_sondes = lines_start_sondes,\n",
    "    colspecs_header = colspecs_header,\n",
    "    header_vars = header_vars,\n",
    "    colspecs_data = colspecs_data,\n",
    "    df_DataRecordFormat = df_DataRecordFormat,\n",
    "    df_HeaderRecordFormat = df_HeaderRecordFormat,\n",
    "    separated_folder = separated_folder,\n",
    "    ID = ID,\n",
    "    max_workers = os.cpu_count() - 1  # Use the cores you want\n",
    ")\n",
    "df_sondes.to_csv(f'{separated_folder}/{ID}__availability.csv',index=False)\n",
    "\n",
    "print_(f'Completed disaggregation of the rawfile of {station} ({ID})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed23939-2842-4196-8745-5c0137503692",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5cc266-b000-449e-9d8d-83675a17e3f9",
   "metadata": {},
   "source": [
    "# 4. Pressure Level Interpolation\n",
    "This section interpolates the processed sonde data onto standardized pressure levels to enable consistent analysis and comparison across different soundings. The interpolation uses a semi-logarithmic approach (linear interpolation in log-pressure space) which is the meteorological standard for atmospheric data. Each sounding is interpolated to regular 5 hPa intervals from 1010 hPa (surface) to 100 hPa (upper troposphere)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6ce9421-b5ef-4a6f-a868-e853d5bb9a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HEADREC</th>\n",
       "      <th>ID</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>RELTIME</th>\n",
       "      <th>NUMLEV</th>\n",
       "      <th>P_SRC</th>\n",
       "      <th>NP_SRC</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15185</th>\n",
       "      <td>#</td>\n",
       "      <td>COM00080222</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9999</td>\n",
       "      <td>38</td>\n",
       "      <td>usaf-ds3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47000</td>\n",
       "      <td>-741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15186</th>\n",
       "      <td>#</td>\n",
       "      <td>COM00080222</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>9999</td>\n",
       "      <td>40</td>\n",
       "      <td>ncdc6322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47000</td>\n",
       "      <td>-741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15187</th>\n",
       "      <td>#</td>\n",
       "      <td>COM00080222</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9999</td>\n",
       "      <td>25</td>\n",
       "      <td>ncdc6322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47000</td>\n",
       "      <td>-741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15188</th>\n",
       "      <td>#</td>\n",
       "      <td>COM00080222</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>9999</td>\n",
       "      <td>34</td>\n",
       "      <td>ncdc6322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47000</td>\n",
       "      <td>-741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15189</th>\n",
       "      <td>#</td>\n",
       "      <td>COM00080222</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9999</td>\n",
       "      <td>4</td>\n",
       "      <td>ncdc6322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47000</td>\n",
       "      <td>-741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24441</th>\n",
       "      <td>#</td>\n",
       "      <td>COM00080222</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>1134</td>\n",
       "      <td>156</td>\n",
       "      <td>ncdc-gts</td>\n",
       "      <td>ncdc-gts</td>\n",
       "      <td>47000</td>\n",
       "      <td>-741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24442</th>\n",
       "      <td>#</td>\n",
       "      <td>COM00080222</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>1701</td>\n",
       "      <td>149</td>\n",
       "      <td>ncdc-gts</td>\n",
       "      <td>ncdc-gts</td>\n",
       "      <td>47000</td>\n",
       "      <td>-741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24443</th>\n",
       "      <td>#</td>\n",
       "      <td>COM00080222</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1131</td>\n",
       "      <td>126</td>\n",
       "      <td>ncdc-gts</td>\n",
       "      <td>ncdc-gts</td>\n",
       "      <td>47000</td>\n",
       "      <td>-741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24444</th>\n",
       "      <td>#</td>\n",
       "      <td>COM00080222</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>1736</td>\n",
       "      <td>133</td>\n",
       "      <td>ncdc-gts</td>\n",
       "      <td>ncdc-gts</td>\n",
       "      <td>47000</td>\n",
       "      <td>-741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24445</th>\n",
       "      <td>#</td>\n",
       "      <td>COM00080222</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>1139</td>\n",
       "      <td>176</td>\n",
       "      <td>ncdc-gts</td>\n",
       "      <td>ncdc-gts</td>\n",
       "      <td>47000</td>\n",
       "      <td>-741500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9261 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HEADREC           ID  YEAR  MONTH  DAY  HOUR  RELTIME  NUMLEV     P_SRC  \\\n",
       "15185       #  COM00080222  1991      1    1     0     9999      38  usaf-ds3   \n",
       "15186       #  COM00080222  1991      1    1    12     9999      40  ncdc6322   \n",
       "15187       #  COM00080222  1991      1    2     0     9999      25  ncdc6322   \n",
       "15188       #  COM00080222  1991      1    2    12     9999      34  ncdc6322   \n",
       "15189       #  COM00080222  1991      1    3     0     9999       4  ncdc6322   \n",
       "...       ...          ...   ...    ...  ...   ...      ...     ...       ...   \n",
       "24441       #  COM00080222  2020     12   29    12     1134     156  ncdc-gts   \n",
       "24442       #  COM00080222  2020     12   29    18     1701     149  ncdc-gts   \n",
       "24443       #  COM00080222  2020     12   30    12     1131     126  ncdc-gts   \n",
       "24444       #  COM00080222  2020     12   30    18     1736     133  ncdc-gts   \n",
       "24445       #  COM00080222  2020     12   31    12     1139     176  ncdc-gts   \n",
       "\n",
       "         NP_SRC    LAT     LON  \n",
       "15185       NaN  47000 -741500  \n",
       "15186       NaN  47000 -741500  \n",
       "15187       NaN  47000 -741500  \n",
       "15188       NaN  47000 -741500  \n",
       "15189       NaN  47000 -741500  \n",
       "...         ...    ...     ...  \n",
       "24441  ncdc-gts  47000 -741500  \n",
       "24442  ncdc-gts  47000 -741500  \n",
       "24443  ncdc-gts  47000 -741500  \n",
       "24444  ncdc-gts  47000 -741500  \n",
       "24445  ncdc-gts  47000 -741500  \n",
       "\n",
       "[9261 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sondes = pd.read_csv(f'{separated_folder}/{ID}__availability.csv')\n",
    "df_sondes = df_sondes.loc[(df_sondes['YEAR']>=clim_start_year)&(df_sondes['YEAR']<=clim_end_year)]\n",
    "df_sondes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ebc3cd-dd4b-44b3-90bb-6a06f35d0768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-28 11:32:29]: Starting interpolation of 9261 soundings to standard pressure levels...\n",
      "[2025-07-28 11:32:33]: Progress: 10% (927/9261 soundings)\n",
      "[2025-07-28 11:32:37]: Progress: 20% (1853/9261 soundings)\n",
      "[2025-07-28 11:32:41]: Progress: 30% (2779/9261 soundings)\n",
      "[2025-07-28 11:32:44]: Progress: 40% (3705/9261 soundings)\n",
      "[2025-07-28 11:32:48]: Progress: 50% (4631/9261 soundings)\n",
      "[2025-07-28 11:32:52]: Progress: 60% (5557/9261 soundings)\n",
      "[2025-07-28 11:32:56]: Progress: 70% (6483/9261 soundings)\n",
      "[2025-07-28 11:32:59]: Progress: 80% (7409/9261 soundings)\n",
      "[2025-07-28 11:33:03]: Progress: 90% (8335/9261 soundings)\n",
      "[2025-07-28 11:33:07]: Progress: 100% (9261/9261 soundings)\n",
      "[2025-07-28 11:33:07]: Interpolation completed! Processed 9261 soundings.\n"
     ]
    }
   ],
   "source": [
    "# Create interpolation template with standard pressure levels\n",
    "df_interpolated = pd.DataFrame(columns=['PRESS', 'GPH', 'TEMP', 'RH', 'DEW', 'U', 'V', 'WSPD', 'WDIR'])\n",
    "press_inf = 1010  # Upper atmosphere limit (hPa)\n",
    "press_sup = 100   # Surface pressure limit (hPa)\n",
    "d_press = 5       # Pressure interval (hPa)\n",
    "df_interpolated['PRESS'] = list(range(press_inf, press_sup - d_press, -d_press))\n",
    "\n",
    "print_(f'Starting interpolation of {len(df_sondes)} soundings to standard pressure levels...')\n",
    "\n",
    "# Progress tracking\n",
    "total_soundings = len(df_sondes.index)\n",
    "completed = 0\n",
    "last_printed_percentage = 0\n",
    "\n",
    "# Process each sounding\n",
    "for i_sounding in df_sondes.index:\n",
    "    \n",
    "    # Copy blank dataframe for this sounding\n",
    "    df_interpolated_i = df_interpolated.copy()\n",
    "    \n",
    "    # Extract date and create filename\n",
    "    date_i = datetime(\n",
    "        int(df_sondes.loc[i_sounding, 'YEAR']),\n",
    "        int(df_sondes.loc[i_sounding, 'MONTH']),\n",
    "        int(df_sondes.loc[i_sounding, 'DAY']),\n",
    "        int(df_sondes.loc[i_sounding, 'HOUR'])\n",
    "    )\n",
    "    \n",
    "    filename_i = f'{ID}_{date_i.strftime(\"%Y%m%d%H%M\")}.csv'\n",
    "    df_sounding_i = pd.read_csv(f'{separated_folder}/{filename_i}')\n",
    "    \n",
    "    df_sounding_i.replace(-9999, np.nan, inplace=True)\n",
    "\n",
    "    if df_sounding_i['PRESS'].isna().all():\n",
    "        df_sondes = df_sondes.drop(i_sounding)\n",
    "        completed += 1\n",
    "        continue\n",
    "    \n",
    "    # Find pressure range for interpolation\n",
    "    max_pressure_i = df_sounding_i['PRESS'].max()\n",
    "    min_pressure_i = df_sounding_i['PRESS'].min()\n",
    "    \n",
    "    maxp_idx = (df_interpolated_i['PRESS'] - max_pressure_i).abs().idxmin()\n",
    "    minp_idx = (df_interpolated_i['PRESS'] - min_pressure_i).abs().idxmin()\n",
    "    \n",
    "    press_interpolated_i = df_interpolated_i['PRESS'].loc[maxp_idx:minp_idx].to_numpy()\n",
    "    \n",
    "    # Interpolate each variable\n",
    "    for var_i in ['GPH', 'TEMP', 'RH', 'DEW', 'U', 'V']:\n",
    "        press_i = df_sounding_i['PRESS'].to_numpy()\n",
    "        var_array_i = df_sounding_i[var_i].to_numpy()\n",
    "        \n",
    "        # Remove NaN values\n",
    "        nan_indices = np.isnan(var_array_i)\n",
    "        press_i = press_i[~nan_indices]\n",
    "        var_array_i = var_array_i[~nan_indices]\n",
    "        \n",
    "        if len(var_array_i) < 5: continue\n",
    "        \n",
    "        # Semi-logarithmic interpolation\n",
    "        var_interpolated_i = np.interp(\n",
    "            np.log(press_interpolated_i), \n",
    "            np.log(press_i[::-1]), \n",
    "            var_array_i[::-1]\n",
    "        )\n",
    "        \n",
    "        df_interpolated_i.loc[maxp_idx:minp_idx, var_i] = var_interpolated_i\n",
    "    \n",
    "    # Calculate wind speed and direction from U and V components\n",
    "    U = df_interpolated_i['U'].astype(float).to_numpy()\n",
    "    V = df_interpolated_i['V'].astype(float).to_numpy()\n",
    "    df_interpolated_i['WSPD'] = np.sqrt(U**2 + V**2) # Wind speed: WSPD = sqrt(U² + V²)\n",
    "    wind_dir_rad = np.arctan2(U, V)\n",
    "    df_interpolated_i['WDIR'] = (np.degrees(wind_dir_rad) + 360) % 360 # Wind direction: meteorological convention (0-360°)\n",
    "    # Handle NaN cases\n",
    "    nan_mask = np.isnan(U) | np.isnan(V)\n",
    "    df_interpolated_i.loc[nan_mask, 'WSPD'] = np.nan\n",
    "    df_interpolated_i.loc[nan_mask, 'WDIR'] = np.nan\n",
    "    \n",
    "    # Export interpolated sounding\n",
    "    df_interpolated_i.to_csv(f'{interpolated_folder}/{filename_i}', index=False)\n",
    "    \n",
    "    # Update progress\n",
    "    completed += 1\n",
    "    current_percentage = (completed * 100) // total_soundings\n",
    "    if current_percentage >= last_printed_percentage + 10 and current_percentage <= 100:\n",
    "        print_(f'Progress: {current_percentage}% ({completed}/{total_soundings} soundings)')\n",
    "        last_printed_percentage = current_percentage\n",
    "\n",
    "df_sondes.to_csv(f'{interpolated_folder}/{ID}__availability.csv',index=False)\n",
    "\n",
    "print_(f'Interpolation completed! Processed {completed} soundings.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
